{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FixMatch-pytorch-on-TPUs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/secutron/FixMatch-pytorch/blob/master/FixMatch_pytorch_on_TPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw62m-MtZmUw"
      },
      "source": [
        "# FixMatch with Pytorch on TPUs - experimental\n",
        "\n",
        "Code: https://github.com/vfdev-5/FixMatch-pytorch/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djEc5_w9Clbu"
      },
      "source": [
        "### Install requirements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gifW7MX3eSaf"
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "\n",
        "try:\n",
        "  import torch_xla\n",
        "  import ignite\n",
        "except ImportError:\n",
        "  VERSION = \"20210304\"    \n",
        "  # VERSION = \"nightly\"\n",
        "  # VERSION = \"20200607\"\n",
        "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "  !python pytorch-xla-env-setup.py --version $VERSION\n",
        "  !pip install --upgrade git+https://github.com/pytorch/ignite\n",
        "  !pip install --upgrade --pre hydra-core\n",
        "  !pip install wandb\n",
        "  !pip uninstall -y pillow && CC=\"cc -mavx2\" pip install --no-cache-dir --force-reinstall pillow-simd\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oicoHubCt_r"
      },
      "source": [
        "### Download dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxMvVxoalCUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661bdae2-0550-4a59-d916-c6abcd7b16a1"
      },
      "source": [
        "# Download dataset:\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "CIFAR10(\"./cifar10\", train=True, download=True);"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./cifar10/cifar-10-python.tar.gz\n",
            "Extracting ./cifar10/cifar-10-python.tar.gz to ./cifar10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS-0uom3CxF0"
      },
      "source": [
        "### Get the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQU8AkGUUnrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7658292-2539-46f7-9981-0030b9cc3d61"
      },
      "source": [
        "!git clone https://github.com/secutron/FixMatch-pytorch/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'FixMatch-pytorch' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-45vgIcDCgj"
      },
      "source": [
        "#### Optionally, login to `W&B`\n",
        "\n",
        "To skip logging to `W&B`, please set `online_exp_tracking.wandb=false` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOTdc6wXU94W"
      },
      "source": [
        "# !wandb login <token>"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGu44tDFDkjQ"
      },
      "source": [
        "### Let's train ResNet18 model in a faster mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poJkPrRCPJDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0407b6d-10fa-4fc2-c787-274e94559eb6"
      },
      "source": [
        "!cd FixMatch-pytorch && export PYTHONPATH=$PWD:$PYTHONPATH && \\\n",
        "  python main_fixmatch.py distributed.backend=xla-tpu distributed.nproc_per_node=8 online_exp_tracking.wandb=true solver.num_epochs=1 \\\n",
        "    ssl.confidence_threshold=0.7 ema_decay=0.9 ssl.cta_update_every=15 solver.optimizer.params.lr=0.1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/hydra/core/default_element.py:127: UserWarning: In 'ssl/cta_pseudo': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  See {url} for more information\"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/core/default_element.py:127: UserWarning: In 'solver/default': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  See {url} for more information\"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/core/default_element.py:127: UserWarning: In 'dataflow/cifar10': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  See {url} for more information\"\"\"\n",
            "2021-08-04 07:41:39,440 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'xla-tpu'\n",
            "2021-08-04 07:41:39,440 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n",
            "\tnproc_per_node: 8\n",
            "\tnnodes: 1\n",
            "\tnode_rank: 0\n",
            "2021-08-04 07:41:39,440 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7f672c0327a0>' in 8 processes\n",
            "2021-08-04 07:42:52,656 FixMatch Training INFO: {'dataflow': {'name': 'cifar10', 'data_path': '/content/cifar10', 'batch_size': 64, 'num_workers': 12}, 'solver': {'num_epochs': 1, 'epoch_length': 128, 'checkpoint_every': 500, 'validate_every': 1, 'resume_from': None, 'optimizer': {'cls': 'torch.optim.SGD', 'params': {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0001, 'nesterov': False}, 'param_groups': {'lr': 0.03, 'momentum': 0.9, 'weight_decay': 0.0001, 'nesterov': False}}, 'supervised_criterion': {'cls': 'torch.nn.CrossEntropyLoss'}, 'lr_scheduler': {'cls': 'torch.optim.lr_scheduler.CosineAnnealingLR', 'params': {'eta_min': 0.0, 'T_max': None}, 'param_groups': {'eta_min': 0.0, 'T_max': None}}, 'unsupervised_criterion': {'cls': 'torch.nn.CrossEntropyLoss', 'params': {'reduction': 'none'}, 'param_groups': {'reduction': 'none'}}}, 'ssl': {'num_train_samples_per_class': 25, 'confidence_threshold': 0.7, 'lambda_u': 1.0, 'mu_ratio': 7, 'cta_update_every': 15}, 'name': 'fixmatch', 'seed': 543, 'debug': False, 'model': 'resnet18', 'num_classes': 10, 'ema_decay': 0.9, 'distributed': {'backend': 'xla-tpu', 'nproc_per_node': 8}, 'online_exp_tracking': {'wandb': True}}\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2021-08-04 07:43:07,484 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<dataflow.Transforme': \n",
            "\t{'batch_size': 8, 'num_workers': 6, 'pin_memory': False, 'drop_last': True, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x7f671faf7290>}\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2021-08-04 07:43:07,498 ignite.distributed.auto.auto_dataloader INFO: DataLoader is wrapped by `MpDeviceLoader` on XLA\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "2021-08-04 07:43:12,054 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<dataflow.Transforme': \n",
            "\t{'batch_size': 8, 'num_workers': 2, 'pin_memory': False, 'drop_last': False, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x7f671fb10e90>}\n",
            "2021-08-04 07:43:12,054 ignite.distributed.auto.auto_dataloader INFO: DataLoader is wrapped by `MpDeviceLoader` on XLA\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "2021-08-04 07:43:18,489 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<dataflow.Transforme': \n",
            "\t{'batch_size': 56, 'num_workers': 6, 'drop_last': True, 'pin_memory': False, 'sampler': <torch.utils.data.distributed.DistributedSampler object at 0x7f671f9f0310>}\n",
            "2021-08-04 07:43:18,489 ignite.distributed.auto.auto_dataloader INFO: DataLoader is wrapped by `MpDeviceLoader` on XLA\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Exception in device=TPU:6: 'DictConfig' object is not callable\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 329, in _mp_start_fn\n",
            "    _start_fn(index, pf_cfg, fn, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n",
            "    fn(gindex, *args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ignite/distributed/comp_models/xla.py\", line 107, in _dist_worker_task_fn\n",
            "    fn(local_rank, *args, **kwargs_dict)\n",
            "  File \"main_fixmatch.py\", line 150, in training\n",
            "    cta_probe_loader=cta_probe_loader,\n",
            "  File \"/content/FixMatch-pytorch/trainers/basic.py\", line 64, in create_trainer\n",
            "    clear_cuda_cache=False,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ignite/contrib/engines/common.py\", line 108, in setup_common_training_handlers\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ignite/contrib/engines/common.py\", line 258, in _setup_common_distrib_training_handlers\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ignite/contrib/engines/common.py\", line 171, in _setup_common_training_handlers\n",
            "    trainer.add_event_handler(Events.ITERATION_STARTED, lr_scheduler)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 308, in add_event_handler\n",
            "    _check_signature(handler, \"handler\", self, *(event_args + args), **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/utils.py\", line 8, in _check_signature\n",
            "    signature = inspect.signature(fn._parent())  # type: ignore[attr-defined]\n",
            "TypeError: 'DictConfig' object is not callable\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Error executing job with overrides: ['distributed.backend=xla-tpu', 'distributed.nproc_per_node=8', 'online_exp_tracking.wandb=true', 'solver.num_epochs=1', 'ssl.confidence_threshold=0.7', 'ema_decay=0.9', 'ssl.cta_update_every=15', 'solver.optimizer.params.lr=0.1']\n",
            "Traceback (most recent call last):\n",
            "  File \"main_fixmatch.py\", line 214, in main\n",
            "    parallel.run(training, cfg)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ignite/distributed/launcher.py\", line 307, in run\n",
            "    idist.spawn(self.backend, func, args=args, kwargs_dict=kwargs, **self._spawn_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ignite/distributed/utils.py\", line 324, in spawn\n",
            "    fn, args=args, kwargs_dict=kwargs_dict, nproc_per_node=nproc_per_node, backend=backend, **kwargs\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ignite/distributed/comp_models/xla.py\", line 128, in spawn\n",
            "    **kwargs,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 394, in spawn\n",
            "    start_method=start_method)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 188, in start_processes\n",
            "    while not context.join():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 144, in join\n",
            "    exit_code=exitcode\n",
            "torch.multiprocessing.spawn.ProcessExitedException: process 6 terminated with exit code 17\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm9LrTRbPI2L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msMIIkIWDddp"
      },
      "source": [
        "### Let's train WRN-28-2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viaP4TVVdGaV"
      },
      "source": [
        "!cd FixMatch-pytorch && export PYTHONPATH=$PWD:$PYTHONPATH && \\\n",
        "  python -u main_fixmatch.py model=WRN-28-2 distributed.backend=xla-tpu distributed.nproc_per_node=8 online_exp_tracking.wandb=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MK5SvZ7DkV4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}